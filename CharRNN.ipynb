{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A-ypM9Rfu35"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "### Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "### Deliverable:\n",
        "- Fill in the code for the RNN (using PyTorch's built-in GRU).\n",
        "- Fill in the training loop\n",
        "- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n",
        "- Implement your own GRU cell.\n",
        "- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n",
        "\n",
        "### Grading Standards:\n",
        "- 20% Implementation the RNN\n",
        "- 20% Implementation training loop\n",
        "- 20% Implementation of evaluation loop\n",
        "- 20% Implementation of your own GRU cell\n",
        "- 20% Training of your RNN on a domain of your choice\n",
        "\n",
        "### Tips:\n",
        "- Read through all the helper functions, run them, and make sure you understand what they are doing\n",
        "- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n",
        "- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n",
        "\n",
        "### Example Output:\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling \n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (You will be implementing the decoder, not the encoder, as we are not doing sequence-to-sequence translation.)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bdZWxvJrsx",
        "outputId": "a5f789c1-c4d4-453d-9f30-59e6c0b20642"
      },
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-10-16 19:50:57--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 52.44.149.188, 54.86.243.162, 52.54.75.23, ...\n",
            "Connecting to piazza.com (piazza.com)|52.44.149.188|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2021-10-16 19:50:57--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 13.35.89.25, 13.35.89.70, 13.35.89.117, ...\n",
            "Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|13.35.89.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-10-16 19:50:58 (31.6 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "file_len = 2579888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxBeKeNjJ0NQ",
        "outputId": "1871fa28-51e9-4d31-a738-1bb1ddbcf64b"
      },
      "source": [
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Lamedon; and the Shadow \n",
            "Host \n",
            "\n",
            "pressed behind and fear went on before them, until they came to Calembel \n",
            "upon Ciril, and the sun went down like blood behind Pinnath Gelin away in \n",
            "the West behind the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On0_WitWJ99e",
        "outputId": "1135b8a0-e999-40f2-ed48-fc3efcc983d9"
      },
      "source": [
        "import torch\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell \n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please do not look at the documentation's code for the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "* Create a custom GRU cell\n",
        "\n",
        "**DONE:**\n",
        "* Create a custom GRU cell\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavAv50ZKQ-F"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.sig = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.W_ir = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_hr = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_iz = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_hz = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_in = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_hn = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)])\n",
        "    \n",
        "  \n",
        "  def forward(self, inputs, hidden):\n",
        "    # Each layer does the following:\n",
        "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "    for i in range(self.num_layers):\n",
        "      r_t = self.sig(self.W_ir[i](inputs) + self.W_hr[i](hidden))\n",
        "      z_t = self.sig(self.W_iz[i](inputs) + self.W_hz[i](hidden))\n",
        "      n_t = self.tanh(self.W_in[i](inputs) + (r_t * self.W_hn[i](hidden)))\n",
        "      h_t = ((1 - z_t) * n_t) + (z_t * (hidden))\n",
        "    return n_t, h_t\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n",
        "**DONE:**\n",
        "* Create an RNN class that extends from nn.Module.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6tNdEnzWj5F"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embedding= nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = GRU(hidden_size, hidden_size, num_layers=n_layers) \n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "    # of the GRU\n",
        "    #print('input_char: ', input_char)\n",
        "    output = self.embedding(input_char).view(1, 1, -1)\n",
        "    output = F.relu(output)\n",
        "    #print('output size: ', output.size())\n",
        "    #print('hidden size: ', hidden.size())\n",
        "    out_decoded, hidden = self.gru(output, hidden)\n",
        "    #print('gru out: ', out_decoded.size())\n",
        "    out_decoded = self.softmax(self.out(out_decoded[0]))\n",
        "    #print('out_dec after softmax size: ', out_decoded.size())\n",
        "    return out_decoded, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrhXghEPKD-5"
      },
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes. \n",
        "\n",
        "**TODO:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "**DONE:**\n",
        "* Fill in the pieces.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ALC3Pf8Kbsi"
      },
      "source": [
        "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n",
        "def train(inp, target):\n",
        "  ## initialize hidden layers, set up gradient and loss \n",
        "    # your code here\n",
        "  ## /\n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden2 = decoder.init_hidden()\n",
        "  loss = 0\n",
        "\n",
        "  input_length = inp.size(0)\n",
        "  target_length = target.size(0)\n",
        "  #print('input_length: ', input_length)\n",
        "  #print('target_length: ', target_length)\n",
        "  \n",
        "  decoder_outputs = torch.zeros(target_length, decoder.hidden_size)\n",
        "  #print('here')\n",
        "  for targ in range(target_length):\n",
        "    #print('inp[input]: ', inp[input])\n",
        "    #print('hidden: ', hidden.size())\n",
        "    decoder_output, hidden2 = decoder(inp[targ], hidden2)\n",
        "    #print('dec_out size: ', decoder_output.size())\n",
        "    #print('target: ', target.unsqueeze(1)[targ])\n",
        "    loss += criterion(decoder_output, target.unsqueeze(1)[targ])\n",
        "  #print('decoder_outputs size: ', decoder_outputs.size())\n",
        "  \n",
        "\n",
        "  #print('here2')\n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item() / target_length\n",
        "    \n",
        "  # more stuff here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n",
        "\n",
        "**DONE:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-bp-OZ1KjNh"
      },
      "source": [
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    # As temperature approaches 0, this sampling function becomes argmax (no randomness)\n",
        "    # As temperature approaches infinity, this sampling function becomes a purely random choice\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.6):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "    # your code here\n",
        "  ## /\n",
        "  with torch.no_grad():\n",
        "    hidden = decoder.init_hidden()\n",
        "    output_string = prime_str\n",
        "  for input_char in char_tensor(prime_str):\n",
        "    #print('TTtWW: ', the_tensor_to_work_with)\n",
        "    decoder_output, hidden = decoder(input_char, hidden)\n",
        "\n",
        "  the_sample = sample_outputs(decoder_output, temperature)\n",
        "  output_string += all_characters[the_sample.item()]\n",
        "  val, idx = decoder_output.data.topk(1)\n",
        "  next_char = idx.squeeze().detach()\n",
        "  \n",
        "  #print('decoder_out size: ', decoder_output.size())\n",
        "  #print('decoder_out: ', decoder_output)\n",
        "  #print('decoder_out argmax: ', torch.argmax(decoder_output))\n",
        "  #print('finished prime_str:', output_string)\n",
        "  for i in range(predict_len - len(prime_str)):\n",
        "    decoder_output, hidden = decoder(next_char, hidden)\n",
        "    #output_string += all_characters[torch.argmax(decoder_output).item()]\n",
        "    the_sample = sample_outputs(decoder_output, temperature)\n",
        "    #print('the_sample: ', all_characters[the_sample.item()])\n",
        "    output_string += all_characters[the_sample.item()]\n",
        "    next_char = the_sample\n",
        "\n",
        "    #output_string += str(decoder_output)\n",
        "    #print('output_string:', output_string)\n",
        "\n",
        "  return output_string\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**TODO:** \n",
        "* Create some cool output\n",
        "\n",
        "**DONE:**\n",
        "* Create some cool output\n",
        "\n",
        "\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string:\n",
        "\n",
        "---\n",
        "\n",
        " G:\n",
        " \n",
        " Gandalf was decrond. \n",
        "'All have lord you. Forward the road at least walk this is stuff, and \n",
        "went to the long grey housel-winding and kindled side was a sleep pleasuring, I do long \n",
        "row hrough. In  \n",
        "\n",
        " lo:\n",
        " \n",
        " lost death it. \n",
        "'The last of the gatherings and take you,' said Aragorn, shining out of the Gate. \n",
        "'Yes, as you there were remembaused to seen their pass, when? What \n",
        "said here, such seven an the sear \n",
        "\n",
        " lo:\n",
        " \n",
        " low, and frod to keepn \n",
        "Came of their most. But here priced doubtless to an Sam up is \n",
        "masters; he left hor as they are looked. And he could now the long to stout in the right fro horseless of \n",
        "the like \n",
        "\n",
        " I:\n",
        " \n",
        " I had been the \n",
        "in his eyes with the perushed to lest, if then only the ring and the legended \n",
        "of the less of the long they which as the \n",
        "enders of Orcovered and smood, and the p \n",
        "\n",
        " I:\n",
        " \n",
        " I they were not the lord of the hoomes. \n",
        "Home already well from the Elves. And he sat strength, and we \n",
        "housed out of the good of the days to the mountains from his perith. \n",
        "\n",
        "'Yess! Where though as if  \n",
        "\n",
        " Th:\n",
        " \n",
        " There yarden \n",
        "you would guard the hoor might. Far and then may was \n",
        "croties, too began to see the drumbred many line \n",
        "and was then hoard walk and they heart, and the chair of the \n",
        "Ents of way, might was \n",
        "\n",
        " G:\n",
        " \n",
        " Gandalf \n",
        "been lat of less the round of the stump; both and seemed to the trees and perished they \n",
        "lay are speered the less; and the wind the steep and have to she \n",
        "precious. There was in the oonly went \n",
        "\n",
        " wh:\n",
        " \n",
        " which went out of the door. \n",
        "Hull the King and of the The days of his brodo \n",
        "stumbler of the windard was a thing there, then it been shining langing \n",
        "to him poor land. They hands; though they seemed ou \n",
        "\n",
        " ra:\n",
        " \n",
        " rather,' have all the least deather \n",
        "down of the truven beginning to the house of sunk. \n",
        "'Nark shorts of the Eyes of the Gate your great nothing as Eret. \n",
        "'I wander trust horn, and there were not, it  \n",
        "\n",
        " I:\n",
        " \n",
        " I can have no mind \n",
        "together! Where don't may had one may little blung \n",
        "terrible to tales. And turn and Gandalf shall be not to as only the Cattring \n",
        "not stopped great the out them forms. On they she lo \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nXFeCmdKodw"
      },
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#print('decoder_hidden: ')\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKfozqw-6eqb",
        "outputId": "34522e30-dbe7-431a-ef5d-8d1cfad58901"
      },
      "source": [
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[79.21881294250488 (200 4%) 2.1122]\n",
            "Whe the heas pirs cand he methe samand the and hime the his at wered mas he the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Ower leros\n",
            "None \n",
            "\n",
            "[157.67925095558167 (400 8%) 2.0617]\n",
            "Wha the and ind the hit and whond of the mongaing arow, and of the apen sand and to the as \n",
            "save was \n",
            "None \n",
            "\n",
            "[234.70628595352173 (600 12%) 1.8477]\n",
            "When the \n",
            "ir now a \n",
            "have \n",
            "seen can it loof. 'We \n",
            "wimh seem the \n",
            "wile all as \n",
            "for and wher \n",
            "whar seen \n",
            "None \n",
            "\n",
            "[312.40771770477295 (800 16%) 1.9726]\n",
            "Wher had was and \n",
            "the shore hay the rome wat and the hear \n",
            "said \n",
            "and her came will sending the had li\n",
            "None \n",
            "\n",
            "[389.9103515148163 (1000 20%) 1.6372]\n",
            "Wh r a or an and for a \n",
            "and of the \n",
            "said; 'bot the wind had came the Elaness spore the come and the d\n",
            "None \n",
            "\n",
            "[466.83579301834106 (1200 24%) 1.4350]\n",
            "Wher the ford have great and and \n",
            "the side and vith the \n",
            "the gate and forst to the come sore the wind\n",
            "None \n",
            "\n",
            "[543.7331745624542 (1400 28%) 1.6040]\n",
            "Whar ere about not of yought the old for the have have allest the Braming leaghing could that Ride,' \n",
            "None \n",
            "\n",
            "[621.7358040809631 (1600 32%) 1.5458]\n",
            "Wher back and the \n",
            "the Compers and \n",
            "bit them the far in the plead light. Some down of the sand of the\n",
            "None \n",
            "\n",
            "[698.3034484386444 (1800 36%) 1.4945]\n",
            "Where the silen were of to the told and the blant least the gross the tome to stood and site and you \n",
            "None \n",
            "\n",
            "[775.0785245895386 (2000 40%) 1.7208]\n",
            "Whi \n",
            "said Aragorn, and he to be \n",
            "that is not you might the begold \n",
            "that his in his forgeth the near d\n",
            "None \n",
            "\n",
            "[851.1522064208984 (2200 44%) 1.5730]\n",
            "What \n",
            "that his tries, as desires of the Tood that was it and the Eats, swirring ret of the \n",
            "The comes\n",
            "None \n",
            "\n",
            "[928.1203911304474 (2400 48%) 1.6847]\n",
            "Whar passed. 'Who long the tume out on the went \n",
            "to as had my Bithout to my will be san of \n",
            "them \n",
            "rou\n",
            "None \n",
            "\n",
            "[1003.6294820308685 (2600 52%) 1.6086]\n",
            "When the oursed to the fees of the \n",
            "heart with a stare in think as folk of \n",
            "the had showed and refece\n",
            "None \n",
            "\n",
            "[1081.2025797367096 (2800 56%) 1.5437]\n",
            "Wher he dack \n",
            "for the din his guess, walk the song for stord have almost passed had good left of your\n",
            "None \n",
            "\n",
            "[1157.5899307727814 (3000 60%) 1.6428]\n",
            "When the streak, and the count of the holden a land. \n",
            "\n",
            "At they \n",
            "passed and fire the minst the singed \n",
            "None \n",
            "\n",
            "[1235.628849029541 (3200 64%) 1.5338]\n",
            "Whit bring of the as were on the than with the darkness and it was to de-paintly were not, \n",
            "for the w\n",
            "None \n",
            "\n",
            "[1312.0565757751465 (3400 68%) 1.3563]\n",
            "Whar you have were still to Him of the woods of the was \n",
            "to be to the worder of the hurrow. The \n",
            "door\n",
            "None \n",
            "\n",
            "[1391.2241051197052 (3600 72%) 1.3439]\n",
            "Whan the hunts were all high and could be it. \n",
            "\n",
            "Frodo east for the black and as of \n",
            "the shadows above\n",
            "None \n",
            "\n",
            "[1467.6309344768524 (3800 76%) 1.4932]\n",
            "When a side words stroke and looking the treemed in his found of the morning and stroked the trees of\n",
            "None \n",
            "\n",
            "[1544.0618181228638 (4000 80%) 1.4928]\n",
            "Whan his \n",
            "down to the ruins of the dark and since and may seemed and made on the forse, not and was w\n",
            "None \n",
            "\n",
            "[1620.6216404438019 (4200 84%) 1.4709]\n",
            "Whet all the Rown to the night of the Surstain to be near the stair. But he was befort the sight in t\n",
            "None \n",
            "\n",
            "[1698.6740064620972 (4400 88%) 1.4615]\n",
            "Where in the morning and pleent been of the fain \n",
            "and and \n",
            "shadows of the \n",
            "white and and \n",
            "down \n",
            "of th\n",
            "None \n",
            "\n",
            "[1775.406744003296 (4600 92%) 1.8584]\n",
            "Where we shadow of one of fire of the sound of away. \n",
            "\n",
            "'I have burneth here of the fire, and filles w\n",
            "None \n",
            "\n",
            "[1853.1483058929443 (4800 96%) 1.6376]\n",
            "Whin an and upon the wind of the hold of the stapped in deep away the very found the end, and the wor\n",
            "None \n",
            "\n",
            "[1930.623664855957 (5000 100%) 1.3830]\n",
            "Whet the things of the dark streast him, and the plains of the \n",
            "even from a land come the \n",
            "moment \n",
            "in\n",
            "None \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "0JBQmrMIjr4o",
        "outputId": "88b92df9-cef8-47dd-d791-1fe8530e2fd3"
      },
      "source": [
        "#a, b = zip(*validations)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(all_losses, label='train')\n",
        "#plt.plot(a, b, label='val')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee0so6aKJ5L8",
        "outputId": "d4e5fc50-89c2-4a81-c83e-7c215f137a7e"
      },
      "source": [
        "# lowest loss achieved: 1.1849\n",
        "# need to hit < 1 for decent performance\n",
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " lo\n",
            " loog Gandalf being spile me and his \n",
            "shall \n",
            "the great my a revining an one he laughters of \n",
            "the Ores to Arvainge of the brange to be seemed any tham the shall the Griders of the rassing to his forge b\n",
            "None \n",
            "\n",
            " he\n",
            " he said to now for a have seem was go lives. Now a while a seemeds stains of his with they wind and been it was still the night and like they was being far longs and the grown forget silen of \n",
            "fell th\n",
            "None \n",
            "\n",
            " I \n",
            " I sas would of when a was every from the red shire. Pippin to field to him hand. If long seemed it him, and a fear of the hope the \n",
            "triend his that while the pitched have seemed the shall light long w\n",
            "None \n",
            "\n",
            " ra\n",
            " ran the \n",
            "would the road on his horse fillow. I fating and was a was a come as it will of the same them, that he will the tirm. \n",
            "\n",
            "Gandring any \n",
            "to seem. And hy \n",
            "lass for the rassone. Me not side to the\n",
            "None \n",
            "\n",
            " lo\n",
            " long. It would stirning of the for the pring. \n",
            "\n",
            "'What have the redouse the plane the rexiled the woods, and the shall for it he said Men for were he saw come seet has shome to his now wold down his su\n",
            "None \n",
            "\n",
            " I \n",
            " I sood be see and for his move a mountour and he red not stand him had no seemed the last some and \n",
            "was to the dark in were him, and stone it was rome of the been \n",
            "who of the could go don't many begon\n",
            "None \n",
            "\n",
            " ca\n",
            " cate of the would at the come to his many to mistly of the Great them see of the Went that his have seemed, and \n",
            "the draince of the mounten the \n",
            "will of the hourre but has on the stard then the road a\n",
            "None \n",
            "\n",
            " ca\n",
            " came of the for the fear. The flowed to \n",
            "\n",
            "him were a find the May the forth falled like the Morder in the \n",
            "stapped any reasing with the words, the below the stered to the more the way he said. 'I sun \n",
            "None \n",
            "\n",
            " Th\n",
            " The thought mespel in the \n",
            "half. They crow and the Shalled in the Horman with a great should, with the home and be that he saw the shorce with the road windows in the \n",
            "was even his even going \n",
            "have ba\n",
            "None \n",
            "\n",
            " ra\n",
            " rac heard again. You hands. \n",
            "\n",
            "'I still he was \n",
            "his last his beat they to glow them.' \n",
            "\n",
            "'I way clown \n",
            "the rised it up any speen the \n",
            "himself to the will quing' said the shot Sam and the fell on the \n",
            "hi\n",
            "None \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
        "\n",
        "**DONE:**\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVYrSJJWHtU9",
        "outputId": "07b2c025-aef4-4b4f-cdf1-85a8c1822535"
      },
      "source": [
        "file = unidecode.unidecode(open('./text_files/alma.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_len = 466656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHxaoEquIP5a",
        "outputId": "0abd415c-5181-4340-d3d4-0805e2218326"
      },
      "source": [
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ther they were overtaken by Antipus we knew not, but I said unto my men: Behold, we know not but they have halted for the purpose that we should come against them, that they might catch us in their sna\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMyTyqNqIQoJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qLX2lHv82tw"
      },
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 600\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#print('decoder_hidden: ')\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to0dRgJHHij3",
        "outputId": "ea6f31e4-bcff-4406-a60c-d2f8a756de04"
      },
      "source": [
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[235.3774597644806 (600 12%) 1.9241]\n",
            "Whe deartion all seep the land to the Lamanites, and by the land of beerating and also the borle to t\n",
            "None \n",
            "\n",
            "[468.800372838974 (1200 24%) 0.9799]\n",
            "Wher had been became among the land of Moroni were said the Lold bethrents of the men of his brithers\n",
            "None \n",
            "\n",
            "[702.4737682342529 (1800 36%) 1.1555]\n",
            "Wher bethren which they were had anto the Nephites and thou hand of Amulek and his brethren unto the \n",
            "None \n",
            "\n",
            "[935.723762512207 (2400 48%) 1.2376]\n",
            "Whes people to the earth, if the Lamanites and their commanded the confulation, and they were sealed \n",
            "None \n",
            "\n",
            "[1168.2617464065552 (3000 60%) 1.0519]\n",
            "Whill know the sin of the wilderness, and those whom all the spirit of the land of Amulek to destruct\n",
            "None \n",
            "\n",
            "[1400.7751903533936 (3600 72%) 0.7921]\n",
            "Whill do not deent unto me down to be brethren, and they say them, and the day of the borth of the re\n",
            "None \n",
            "\n",
            "[1634.09024477005 (4200 84%) 1.1087]\n",
            "What astentions of the farom of the land.\n",
            "\n",
            " And thus we have for the south of the city of rightly did\n",
            "None \n",
            "\n",
            "[1867.7111508846283 (4800 96%) 1.1704]\n",
            "Whords, behold, when he should be said unto the gare of year of the land of years and the land of Mor\n",
            "None \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hWep23qQVY1",
        "outputId": "6a166147-50dd-450f-a436-1d004ccef242"
      },
      "source": [
        "string_list = ['An', 'B', 'Th', 'F', 'At', 'Be', 'K', 'W', 'Z', 'Su', 'Tr', 'M', 'N', 'O', 'P']\n",
        "for i in range(1, 16):\n",
        "  prime_str = string_list[i-1]\n",
        "  generated_sentence = evaluate(prime_str, 200) \n",
        "  print('{} ({}): {}'.format(i, prime_str, generated_sentence))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 (An): And they said unto the men of the hearts of the land of Zarahemnances, and they brethren, and they had spoken unto them who were the word of Ammon and also the land of Zarahemla, to the church.\n",
            "\n",
            " And t\n",
            "2 (B): B lek with attent therefore the people of Nephi.\n",
            "\n",
            " And they was a time get them that they might slay in the true of his men therefore they did came to pass that this things that they are no more who we\n",
            "3 (Th): The sacred out of the war and therefore, now, the Lamanites can our manner of the judges of the judges.\n",
            "\n",
            " And now behold, they did not that they shall not on the church, and with the judges, and are no\n",
            "4 (F): Fore--\n",
            "\n",
            " And now it came to pass that the for they went the wilderness of their fathers and strangence.\n",
            "\n",
            " And thus they are not the state were with the band of the servants of the Lamanites, and also s\n",
            "5 (At): Ath the church, by the judges over the city of Manous, and his people of the take antance of their prisoners, and our hand into the same which had received up unto the land of Zarahemla, and sent to ma\n",
            "6 (Be): Behold, ye have cated unto repentance.\n",
            "\n",
            " And it came to pass that they shall received their will not all the wilderness that they have destroyed, which he would distrueth unto the great shanner of the \n",
            "7 (K): K the death year of the judges.\n",
            "\n",
            " And it came to pass that their being life these will seast according to this were destroyed and preparted unto the hearts of the wert of the true of the river Sidon an\n",
            "8 (W): W his mortance of the land of Nephi was repent in the church which was for their armies, who were spoken that they were are land unto the land of Zarahemen, we would be the cause of the resurrection of\n",
            "9 (Z): Z the reign, in the spirits of the city and the word of the Lamanites should be no more, for they shall not suppose his men unto the land of Maron.\n",
            "\n",
            " And now it came to pass that they were commenced of\n",
            "10 (Su): Sunt the church which were from the church.\n",
            "\n",
            " And now, it was also every with his people.\n",
            "\n",
            " And it came to pass that these things which was not consion onter the land of Zarahemla, and because of the j\n",
            "11 (Tr): Troad of the people of Moroni was a surety the reign of the explets of the people of Moroni will not look and the hearts of the Lamanites, who were deliver unto the word of the ton the time who were al\n",
            "12 (M): M the Lord their wass bring the spirits of the justice in the cause of the cause of the judges of the holy spitiest.\n",
            "\n",
            " And Moroni with must that he who had many with their father, that they were all no\n",
            "13 (N): Nephi, and the reign of the plant of the rive of the people of Moroni to the take of your great end of the Lamanites shall be destroyed that they were preparated them from the land of Ammon and the com\n",
            "14 (O): O did not caused that they might that ye shall be the men of the wilderness of the wilderness of their fathers, in which was care their lands were come unto your into the land of Zarahemla, and the peo\n",
            "15 (P): Prthey were armies and his sprepart the judges of the Lord the wilderness which had faith and as believed that they murdered them for their arms and the church, and also that Moroni and their cappedien\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqS1cTceSgcj"
      },
      "source": [
        "It seems to me that the model does will given a starting string of something that it had seen before. This is evident by #6. It begins with \"Behold\" which I am sure occurs often in the text. However, I don't think that it can generalize very well. For example, there are many cases where it just prints out the letter that it's not used to seeing, and then will generate text unrelated."
      ]
    }
  ]
}